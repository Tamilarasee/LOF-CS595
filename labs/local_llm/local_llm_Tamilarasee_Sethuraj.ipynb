{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 1\n",
    "**Name** - Tamilarasee Sethuraj\n",
    "\n",
    "**CWID** - A20553416\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Download Models  \n",
    "\n",
    "Meta-Llama-3-8B-Instruct-Q4_K_S.gguf (https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/blob/main/Meta-Llama-3-8B-Instruct-Q4_K_S.gguf)\n",
    "\n",
    "and Phi-3-mini-4k-instruct.Q6_K.gguf(https://huggingface.co/bartowski/Phi-3-mini-4k-instruct-GGUF/blob/main/Phi-3-mini-4k-instruct-Q6_K.gguf)\n",
    "\n",
    "and place them in <Project Root>/labs/local_llm folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell once to set up your conversation. \n",
    "\n",
    "# Set up your assistant prompt and initialize the message thread.\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Answer the user's question from the perspective of THE world's leading applicable domain knowledge expert. Process: 1. Rephrase the question \n",
    "to anticipate the user's needs. 2. Break the question into subquestions. 2. Assemble answers in a step-by-step manner organized for optimal comprehension. 3. Include Google Scholar and Google links\n",
    "formatted as follows, that is, do not include direct links to articles, instead search for related terms as follows.:\n",
    "\n",
    "- _See also:_ [Related topics for deeper understanding]\n",
    "  üìö[Research topic articles](https://scholar.google.com/scholar?q=related+terms)\n",
    "  üîç[General information](https://www.google.com/search?q=related+terms)\n",
    "\n",
    "- _You may also enjoy:_ [Topics of tangential interest]\n",
    "  üåü[Explore more](https://www.google.com/search?q=tangential+interest+terms)\n",
    "\n",
    "\"\"\"\n",
    "message_thread = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add your first and subsequent questions here! Rerun this cell and the next. \n",
    "\n",
    "user_prompt = \"How can I avoid overeating?\"\n",
    "\n",
    "\n",
    "# Define the output file path for the conversation\n",
    "output_file = \"output.md\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 '√Ñ'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4467.80 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'Meta-Llama-3-8B-Instruct', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '14', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.entries_count': '224', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'quantize.imatrix.chunks_count': '88', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "\n",
      "llama_print_timings:        load time =   90317.20 ms\n",
      "llama_print_timings:      sample time =     488.85 ms /   300 runs   (    1.63 ms per token,   613.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   90316.57 ms /   212 tokens (  426.02 ms per token,     2.35 tokens per second)\n",
      "llama_print_timings:        eval time =  205619.47 ms /   299 runs   (  687.69 ms per token,     1.45 tokens per second)\n",
      "llama_print_timings:       total time =  309963.48 ms /   511 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# The following function dynamically updates the output file as responses are generated.\n",
    "\n",
    "def clear_output_file():\n",
    "    \"\"\"\n",
    "    Clears all content from the output file by writing an empty string\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\")  # Clear the file by writing an empty string\n",
    "\n",
    "\n",
    "def write_to_file(content):\n",
    "    \"\"\"\n",
    "    Appends new content to the output file\n",
    "    Args:\n",
    "        content: Text content to append to the file\n",
    "    \"\"\"\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    # If the file is not already open in VSCode, uncomment the next line\n",
    "    # os.system(f\"code {output_file}\"). \n",
    "    # Best, though, to have the file open and in preview mode. Then you see output\n",
    "    # as it is generated!\n",
    "    \n",
    "    # Add the latest prompt to the thread\n",
    "message_thread.append({\"role\": \"user\", \"content\": user_prompt}) \n",
    "\n",
    "\n",
    "# Append the Markdown content to a file\n",
    "with open(output_file, \"a\", encoding=\"utf-8\") as file:\n",
    "    file.write(f'\\n\\n _____ \\n\\n User üë©‚Äç‚öïÔ∏è: {user_prompt} \\n\\n ')\n",
    "\n",
    "llm = Llama(\n",
    "      model_path =\"./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\",\n",
    "      chat_format = \"llama-3\",\n",
    "      verbose=True,\n",
    "      n_gpu_layers=0, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    "  )\n",
    "output = llm.create_chat_completion(\n",
    "        messages = message_thread,\n",
    "        max_tokens=1000,\n",
    "        stream=True,\n",
    "        stop = \"<|assistant|>\",\n",
    "        )\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for chunk in output:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'role' in delta:\n",
    "        role_text = delta['role'] + \"üëΩ\" + \": \"\n",
    "        # print(role_text, end='')\n",
    "        text += role_text\n",
    "        write_to_file(role_text)\n",
    "    elif 'content' in delta:\n",
    "        content_text = delta['content']\n",
    "        # print(content_text, end='')\n",
    "        text += content_text\n",
    "        write_to_file(content_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If the context window is big enough, add the latest response to the thread. You can then go back to update the user prompt and send again.\n",
    "message_thread.append({\"role\": \"system\", \"content\": text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"Answer the user's question from the perspective of THE world's leading applicable domain knowledge expert. Process: 1. Rephrase the question \\nto anticipate the user's needs. 2. Break the question into subquestions. 2. Assemble answers in a step-by-step manner organized for optimal comprehension. 3. Include Google Scholar and Google links\\nformatted as follows, that is, do not include direct links to articles, instead search for related terms as follows.:\\n\\n- _See also:_ [Related topics for deeper understanding]\\n  üìö[Research topic articles](https://scholar.google.com/scholar?q=related+terms)\\n  üîç[General information](https://www.google.com/search?q=related+terms)\\n\\n- _You may also enjoy:_ [Topics of tangential interest]\\n  üåü[Explore more](https://www.google.com/search?q=tangential+interest+terms)\\n\\n\"}, {'role': 'user', 'content': 'How can I avoid overeating?'}, {'role': 'system', 'content': \"assistantüëΩ: A crucial question for maintaining a healthy lifestyle! Let's break it down into smaller, manageable chunks to help you develop strategies to avoid overeating.\\n\\n**Rephrased question:** How can I cultivate mindful eating habits and control my food intake to maintain a balanced diet?\\n\\n**Subquestions:**\\n\\n1. What are some common triggers that lead to overeating?\\n2. How can I identify and manage emotional hunger?\\n3. What are some effective strategies for slowing down while eating?\\n4. Can you recommend some healthy snacking options?\\n5. Are there any specific foods or nutrients that can help with satiety?\\n\\n**Step-by-step answers:**\\n\\n1. **Common triggers:** Overeating is often triggered by stress, boredom, emotional states (e.g., anxiety, sadness), and social pressures. Identify your personal triggers to develop strategies for coping with them.\\n\\n_See also:_ Emotional eating, mindful eating, stress management\\nüìö[Emotional Eating](https://scholar.google.com/scholar?q=emotional+eating)\\nüîç[Stress Management Techniques](https://www.google.com/search?q=stress+management+techniques)\\n\\n2. **Identifying and managing emotional hunger:** Pay attention to your body's physical cues (hunger, fullness) and emotional states. Ask yourself:\\n\\t* Am I truly hungry or just bored/stressed/emotional?\\n\\t* Can I delay eating for a short period?\\n\\n_See also:_ Mindful eating, emotional\"}]\n"
     ]
    }
   ],
   "source": [
    "print(message_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_1 = \"Can you recommend some healthy snacks to curb cravings?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 '√Ñ'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4467.80 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'Meta-Llama-3-8B-Instruct', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '14', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.entries_count': '224', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'quantize.imatrix.chunks_count': '88', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "\n",
      "llama_print_timings:        load time =  154776.18 ms\n",
      "llama_print_timings:      sample time =     851.63 ms /   547 runs   (    1.56 ms per token,   642.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =  165104.92 ms /   537 tokens (  307.46 ms per token,     3.25 tokens per second)\n",
      "llama_print_timings:        eval time =  281459.67 ms /   546 runs   (  515.49 ms per token,     1.94 tokens per second)\n",
      "llama_print_timings:       total time =  479069.32 ms /  1083 tokens\n"
     ]
    }
   ],
   "source": [
    "message_thread.append({\"role\": \"user\", \"content\": user_prompt_1}) \n",
    "\n",
    "\n",
    "# Append the Markdown content to a file\n",
    "with open(output_file, \"a\", encoding=\"utf-8\") as file:\n",
    "    file.write(f'\\n\\n _____ \\n\\n User üë©‚Äç‚öïÔ∏è: {user_prompt_1} \\n\\n ')\n",
    "\n",
    "llm = Llama(\n",
    "      model_path =\"./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\",\n",
    "      chat_format = \"llama-3\",\n",
    "      verbose=True,\n",
    "      n_gpu_layers=0, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      n_ctx=2048, # Uncomment to increase the context window\n",
    "  )\n",
    "output = llm.create_chat_completion(\n",
    "        messages = message_thread,\n",
    "        max_tokens=1000,\n",
    "        stream=True,\n",
    "        stop = \"<|assistant|>\",\n",
    "        )\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for chunk in output:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'role' in delta:\n",
    "        role_text = delta['role'] + \"üëΩ\" + \": \"\n",
    "        # print(role_text, end='')\n",
    "        text += role_text\n",
    "        write_to_file(role_text)\n",
    "    elif 'content' in delta:\n",
    "        content_text = delta['content']\n",
    "        # print(content_text, end='')\n",
    "        text += content_text\n",
    "        write_to_file(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Llama.n_ctx of <llama_cpp.llama.Llama object at 0x000002333001F150>>\n"
     ]
    }
   ],
   "source": [
    "print(llm.n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, if you want to clear your output!\n",
    "\n",
    "\n",
    "clear_output_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Students will need to fill in the critical functions\n",
    "# Hints are provided within the comments.\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Operating system interface - for file and path operations\n",
    "import os\n",
    "\n",
    "# NumPy - Scientific computing library for array operations and numerical computations\n",
    "import numpy as np\n",
    "\n",
    "# PDF processing library - for reading and extracting text from PDF files\n",
    "import PyPDF2\n",
    "\n",
    "# Sentence Transformers - for creating semantic embeddings from text\n",
    "# (Note: This import appears unused in the current code as you're using Llama for embeddings)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "# Facebook AI Similarity Search (FAISS) - for efficient similarity search and clustering of dense vectors\n",
    "# Used here for storing and querying text embeddings\n",
    "import faiss\n",
    "\n",
    "# Python bindings for the llama.cpp library - provides access to Llama language models\n",
    "# Used for text generation and creating embeddings\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_file = \"Sample Patient 1.pdf\" \n",
    "\n",
    "# Create a PDF reader object to read the file\n",
    "pdf_reader = PyPDF2.PdfReader(open(pdf_file, \"rb\"))\n",
    "pdf_text = \"\"\n",
    "\n",
    "# Iterate through each page of the PDF\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    # Extract text from the current page\n",
    "    page_text = pdf_reader.pages[page_num].extract_text()\n",
    "    # If text was successfully extracted, add it to our accumulated text\n",
    "    if page_text:\n",
    "        pdf_text += page_text + \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical\n",
      "Profile:\n",
      "Patient\n",
      "Demographics:\n",
      "‚óè\n",
      "Age:\n",
      "57\n",
      "years\n",
      "‚óè\n",
      "Gender:\n",
      "Male\n",
      "‚óè\n",
      "Ethnicity:\n",
      "Non-Hispanic\n",
      "‚óè\n",
      "BMI:\n",
      "31\n",
      "kg/m¬≤\n",
      "(Obese)\n",
      "Medical\n",
      "History:\n",
      "‚óè\n",
      "Primary\n",
      "Condition:\n",
      "Type\n",
      "2\n",
      "Diabetes\n",
      "Mellitus\n",
      "(T2DM)\n",
      "(Diagnosed\n",
      "12\n",
      "years\n",
      "ago)\n",
      "‚óè\n",
      "Glycemic\n",
      "Control:\n",
      "‚óã\n",
      "Last\n",
      "HbA1c:\n",
      "8.3%\n",
      "(Above\n",
      "target)\n",
      "‚óã\n",
      "Fasting\n",
      "Glucose:\n",
      "160\n",
      "mg/dL\n",
      "‚óè\n",
      "Medications:\n",
      "‚óã\n",
      "Metformin\n",
      "1000\n",
      "mg\n",
      "twice\n",
      "daily\n",
      "‚óã\n",
      "Insulin\n",
      "Glargine\n",
      "20\n",
      "units\n",
      "at\n",
      "bedtime\n",
      "‚óã\n",
      "Lisinopril\n",
      "10\n",
      "mg\n",
      "once\n",
      "daily\n",
      "for\n",
      "hypertension\n",
      "‚óã\n",
      "Atorvastatin\n",
      "40\n",
      "mg\n",
      "once\n",
      "daily\n",
      "for\n",
      "hyperlipidemia\n",
      "‚óè\n",
      "Non-adherence\n",
      "Issues:\n",
      "Occasionally\n",
      "misses\n",
      "insulin\n",
      "doses\n",
      "(2‚Äì3\n",
      "days\n",
      "per\n",
      "week),\n",
      "poor\n",
      "diet\n",
      "management\n",
      "‚óè\n",
      "Comorbidities:\n",
      "‚óã\n",
      "Hypertension:\n",
      "Controlled\n",
      "with\n",
      "medication\n",
      "‚óã\n",
      "Dyslipidemia:\n",
      "Partially\n",
      "controlled\n",
      "(LDL\n",
      "110\n",
      "mg/dL)\n",
      "‚óã\n",
      "Obesity:\n",
      "BMI\n",
      "of\n",
      "31\n",
      "kg/m¬≤,\n",
      "limited\n",
      "physical\n",
      "activity\n",
      "‚óã\n",
      "Diabetic\n",
      "Retinopathy:\n",
      "Mild\n",
      "background\n",
      "retinopathy\n",
      "(diagnosed\n",
      "2\n",
      "years\n",
      "ago)\n",
      "‚óã\n",
      "Diabetic\n",
      "Neuropathy:\n",
      "Tingling\n",
      "and\n",
      "numbness\n",
      "in\n",
      "feet\n",
      "‚óã\n",
      "Chronic\n",
      "Kidney\n",
      "Disease:\n",
      "Stage\n",
      "2\n",
      "(GFR\n",
      "65\n",
      "mL/min/1.73\n",
      "m¬≤)\n",
      "Social\n",
      "History:\n",
      "‚óè\n",
      "Smoking:\n",
      "Non-smoker\n",
      "‚óè\n",
      "Alcohol:\n",
      "Drinks\n",
      "occasionally\n",
      "‚óè\n",
      "Physical\n",
      "Activity:\n",
      "Sedentary,\n",
      "walks\n",
      "for\n",
      "10\n",
      "minutes\n",
      "a\n",
      "day\n",
      "Lifestyle\n",
      "Factors:\n",
      "‚óè\n",
      "Diet:\n",
      "Consumes\n",
      "a\n",
      "diet\n",
      "high\n",
      "in\n",
      "carbohydrates,\n",
      "irregular\n",
      "meal\n",
      "patterns\n",
      "‚óè\n",
      "Exercise:\n",
      "Very\n",
      "low\n",
      "activity\n",
      "level,\n",
      "finds\n",
      "it\n",
      "hard\n",
      "to\n",
      "incorporate\n",
      "physical\n",
      "activity\n",
      "into\n",
      "daily\n",
      "life\n",
      "Patient\n",
      "Health\n",
      "Assessment\n",
      "Questionnaire:\n",
      "1.\n",
      "Blood\n",
      "Sugar\n",
      "Monitoring:\n",
      "‚óã\n",
      "How\n",
      "often\n",
      "do\n",
      "you\n",
      "check\n",
      "your\n",
      "blood\n",
      "sugar\n",
      "levels\n",
      "at\n",
      "home?\n",
      "2.\n",
      "Physical\n",
      "Activity:\n",
      "‚óã\n",
      "How\n",
      "many\n",
      "days\n",
      "per\n",
      "week\n",
      "do\n",
      "you\n",
      "engage\n",
      "in\n",
      "physical\n",
      "activity,\n",
      "and\n",
      "what\n",
      "kind\n",
      "of\n",
      "activities\n",
      "do\n",
      "you\n",
      "do?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of words in pdf =  234\n",
      "Number of text chunks: 1\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=500, chunk_overlap=50) -> list:\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks for processing\n",
    "    Args:\n",
    "        text: Input text to be chunked\n",
    "        chunk_size: Number of words per chunk\n",
    "        chunk_overlap: Number of overlapping words between chunks\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    HINTS:\n",
    "      - Split the input text into a list of words.\n",
    "      - Use a while loop to keep creating chunks until you've processed all words.\n",
    "      - For each chunk:\n",
    "        1. Calculate the end position based on `chunk_size`.\n",
    "        2. Join the words from `start` to `end` into a single string.\n",
    "        3. Append this chunk to a list.\n",
    "        4. Adjust `start` by subtracting `chunk_overlap` to ensure overlapping words.\n",
    "      - Make sure `start` never goes below zero.\n",
    "      - Return the list of chunks.\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    pdf_words = text.split()\n",
    "    print(\"No of words in pdf = \",len(pdf_words))\n",
    "    start = 0    \n",
    "    end = chunk_size\n",
    "    chunks = []\n",
    "    if len(pdf_words) <= chunk_size:\n",
    "        chunks.append(\" \".join(pdf_words))\n",
    "        return chunks\n",
    "    while end <= len(pdf_words):\n",
    "        chunk = \" \".join(pdf_words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - chunk_overlap\n",
    "        end = min(start + chunk_size, len(pdf_words))    \n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Process the PDF text into chunks and print the total number of chunks created\n",
    "text_chunks = chunk_text(pdf_text, chunk_size=500, chunk_overlap=50)\n",
    "print(f\"Number of text chunks: {len(text_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama model loaded with embedding support.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\"\n",
    "\n",
    "llm_for_embeddings = Llama(\n",
    "    model_path=model_path,    # Path to the Llama model file\n",
    "    n_ctx=2048,              # Context window size\n",
    "    embedding=True,          # Enable embedding generation\n",
    "    verbose=False            # Disable verbose output\n",
    ")\n",
    "\n",
    "print(\"Llama model loaded with embedding support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical Profile: Patient Demographics: ‚óè Age: 57 years ‚óè Gender: Male ‚óè Ethnicity: Non-Hispanic ‚óè BMI: 31 kg/m¬≤ (Obese) Medical History: ‚óè Primary Condition: Type 2 Diabetes Mellitus (T2DM) (Diagnosed 12 years ago) ‚óè Glycemic Control: ‚óã Last HbA1c: 8.3% (Above target) ‚óã Fasting Glucose: 160 mg/dL ‚óè Medications: ‚óã Metformin 1000 mg twice daily ‚óã Insulin Glargine 20 units at bedtime ‚óã Lisinopril 10 mg once daily for hypertension ‚óã Atorvastatin 40 mg once daily for hyperlipidemia ‚óè Non-adherence Issues: Occasionally misses insulin doses (2‚Äì3 days per week), poor diet management ‚óè Comorbidities: ‚óã Hypertension: Controlled with medication ‚óã Dyslipidemia: Partially controlled (LDL 110 mg/dL) ‚óã Obesity: BMI of 31 kg/m¬≤, limited physical activity ‚óã Diabetic Retinopathy: Mild background retinopathy (diagnosed 2 years ago) ‚óã Diabetic Neuropathy: Tingling and numbness in feet ‚óã Chronic Kidney Disease: Stage 2 (GFR 65 mL/min/1.73 m¬≤) Social History: ‚óè Smoking: Non-smoker ‚óè Alcohol: Drinks occasionally ‚óè Physical Activity: Sedentary, walks for 10 minutes a day Lifestyle Factors: ‚óè Diet: Consumes a diet high in carbohydrates, irregular meal patterns ‚óè Exercise: Very low activity level, finds it hard to incorporate physical activity into daily life Patient Health Assessment Questionnaire: 1. Blood Sugar Monitoring: ‚óã How often do you check your blood sugar levels at home? 2. Physical Activity: ‚óã How many days per week do you engage in physical activity, and what kind of activities do you do?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical Profile: Patient Demographics: ‚óè Age: 57 years ‚óè Gender: Male ‚óè Ethnicity: Non-Hispanic ‚óè BMI: 31 kg/m¬≤ (Obese) Medical History: ‚óè Primary Condition: Type 2 Diabetes Mellitus (T2DM) (Diagnosed 12 years ago) ‚óè Glycemic Control: ‚óã Last HbA1c: 8.3% (Above target) ‚óã Fasting Glucose: 160 mg/dL ‚óè Medications: ‚óã Metformin 1000 mg twice daily ‚óã Insulin Glargine 20 units at bedtime ‚óã Lisinopril 10 mg once daily for hypertension ‚óã Atorvastatin 40 mg once daily for hyperlipidemia ‚óè Non-adherence Issues: Occasionally misses insulin doses (2‚Äì3 days per week), poor diet management ‚óè Comorbidities: ‚óã Hypertension: Controlled with medication ‚óã Dyslipidemia: Partially controlled (LDL 110 mg/dL) ‚óã Obesity: BMI of 31 kg/m¬≤, limited physical activity ‚óã Diabetic Retinopathy: Mild background retinopathy (diagnosed 2 years ago) ‚óã Diabetic Neuropathy: Tingling and numbness in feet ‚óã Chronic Kidney Disease: Stage 2 (GFR 65 mL/min/1.73 m¬≤) Social History: ‚óè Smoking: Non-smoker ‚óè Alcohol: Drinks occasionally ‚óè Physical Activity: Sedentary, walks for 10 minutes a day Lifestyle Factors: ‚óè Diet: Consumes a diet high in carbohydrates, irregular meal patterns ‚óè Exercise: Very low activity level, finds it hard to incorporate physical activity into daily life Patient Health Assessment Questionnaire: 1. Blood Sugar Monitoring: ‚óã How often do you check your blood sugar levels at home? 2. Physical Activity: ‚óã How many days per week do you engage in physical activity, and what kind of activities do you do?\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(chunk)\n",
    "    embed_output = llm_for_embeddings.create_embedding(chunk)\n",
    "\n",
    "print(type(embed_output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['object', 'data', 'model', 'usage'])\n"
     ]
    }
   ],
   "source": [
    "print(embed_output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['object', 'embedding', 'index'])\n"
     ]
    }
   ],
   "source": [
    "print(embed_output['data'][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llama_embed_text(text)-> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates embeddings for input text using Llama model\n",
    "    Args:\n",
    "        text: Input text to embed\n",
    "    Returns:\n",
    "        Numpy array containing the mean embedding vector\n",
    "      HINTS:\n",
    "      1) Generate embeddings for the input text using the Llama model.\n",
    "      2) Use llm_for_embeddings.create_embedding(...)\n",
    "      3) Extract the embedding vectors from the result.\n",
    "      4) Convert the embeddings to a numpy array for numerical operations.\n",
    "      5) Calculate the mean of all token embeddings to get a single vector representation.\n",
    "      6) Return this mean vector as the embedding for the input text.\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    embed_output = llm_for_embeddings.create_embedding(text)\n",
    "    embedded_output  = np.array(embed_output[\"data\"][0][\"embedding\"])   \n",
    "    return np.mean(embedded_output,axis=0)\n",
    "#axis=0 helps to summaize/take mean of all tokens(rows) in the chunk for each dimension(context representation)\n",
    "\n",
    "# Process embeddings for all text chunks\n",
    "all_embeddings = []\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    # TODO: Complete the Loop Code\n",
    "    # 1. Generate embeddings for current chunk using llama_embed_text(chunk)\n",
    "    # 2. Append the resulting embedding vector to all_embeddings list\n",
    "    chunk_embeded = llama_embed_text(chunk)\n",
    "    all_embeddings.append(chunk_embeded)    \n",
    "\n",
    "# Stack all embeddings into a single matrix of shape (N, 4096)\n",
    "embeddings_matrix = np.vstack(all_embeddings).astype(\"float32\")\n",
    "\n",
    "# Initialize FAISS index for similarity search\n",
    "index = faiss.IndexFlatL2(4096)\n",
    "#Facebook AI Similarity Search  - for efficient similarity search and clustering of dense vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17072605 -0.5408788  -1.9530199  ...  0.0487665  -0.20802855\n",
      "   0.97837037]]\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_matrix)\n",
    "print(len(embeddings_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new FAISS index using L2 (Euclidean) distance metric\n",
    "# embeddings_matrix.shape[1] specifies the dimensionality of our vectors (4096 in this case)\n",
    "index = faiss.IndexFlatL2(embeddings_matrix.shape[1])\n",
    "\n",
    "# Add all our document embeddings to the index\n",
    "index.add(embeddings_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0747873e+04 3.4028235e+38 3.4028235e+38]] [[ 0 -1 -1]]\n",
      "--- Result 1 (distance 10747.8730) ---\n",
      "Clinical Profile: Patient Demographics: ‚óè Age: 57 years ‚óè Gender: Male ‚óè Ethnicity: Non-Hispanic ‚óè BMI: 31 kg/m¬≤ (Obese) Medical History: ‚óè Primary Condition: Type 2 Diabetes Mellitus (T2DM) (Diagnosed 12 years ago) ‚óè Glycemic Control: ‚óã Last HbA1c: 8.3% (Above target) ‚óã Fasting Glucose: 160 mg/dL ‚óè Medications: ‚óã Metformin 1000 mg twice daily ‚óã Insulin Glargine 20 units at bedtime ‚óã Lisinopril 10 mg once daily for hypertension ‚óã Atorvastatin 40 mg once daily for hyperlipidemia ‚óè Non-adherence Issues: Occasionally misses insulin doses (2‚Äì3 days per week), poor diet management ‚óè Comorbidities: ‚óã Hypertension: Controlled with medication ‚óã Dyslipidemia: Partially controlled (LDL 110 mg/dL) ‚óã Obesity: BMI of 31 kg/m¬≤, limited physical activity ‚óã Diabetic Retinopathy: Mild background retinopathy (diagnosed 2 years ago) ‚óã Diabetic Neuropathy: Tingling and numbness in feet ‚óã Chronic Kidney Disease: Stage 2 (GFR 65 mL/min/1.73 m¬≤) Social History: ‚óè Smoking: Non-smoker ‚óè Alcohol: Drinks occasionally ‚óè Physical Activity: Sedentary, walks for 10 minutes a day Lifestyle Factors: ‚óè Diet: Consumes a diet high in carbohydrates, irregular meal patterns ‚óè Exercise: Very low activity level, finds it hard to incorporate physical activity into daily life Patient Health Assessment Questionnaire: 1. Blood Sugar Monitoring: ‚óã How often do you check your blood sugar levels at home? 2. Physical Activity: ‚óã How many days per week do you engage in physical activity, and what kind of activities do you do?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_similar_chunks(query, k=3) -> list:\n",
    "    \"\"\"\n",
    "    Searches for text chunks similar to the query using FAISS\n",
    "    Args:\n",
    "        query: Search query text\n",
    "        k: Number of results to return\n",
    "    Returns:\n",
    "        List of tuples containing (text_chunk, distance)\n",
    "    HINTS:\n",
    "      1) Convert 'query' to an embedding.\n",
    "      2) Call index.search(...) to find the top k matches.\n",
    "      3) Return a list of (text_chunk, distance) pairs.\n",
    "      4) Make sure 'query_emb' is reshaped to (1, -1) before searching.\n",
    "      5) Use the indices from index.search to retrieve the chunk texts.\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    query_emb = llama_embed_text(query).reshape(1,-1)\n",
    "    distances, indices = index.search(query_emb,k)\n",
    "    result =[]\n",
    "    print(distances, indices)\n",
    "    for i in range(k):\n",
    "        j = indices[0][i]\n",
    "        if j != -1:\n",
    "            result.append((text_chunks[j], distances[0][i]))\n",
    "    return result\n",
    "\n",
    "# Example usage of the search function\n",
    "test_query = \"How can I avoid overeating?\"\n",
    "search_results = search_similar_chunks(test_query, k=3)\n",
    "\n",
    "# Print each result with its similarity score\n",
    "for i, (chunk, dist) in enumerate(search_results):\n",
    "    print(f\"--- Result {i+1} (distance {dist:.4f}) ---\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 '√Ñ'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4467.80 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'Meta-Llama-3-8B-Instruct', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '14', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.entries_count': '224', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'quantize.imatrix.chunks_count': '88', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Llama model for chat interactions\n",
    "llm_for_chat = Llama(\n",
    "    model_path=model_path,    # Path to the Llama model weights file\n",
    "    n_ctx=2048,              # Maximum context window size (in tokens)\n",
    "    verbose=True             # Enable detailed logging output\n",
    ")\n",
    "\n",
    "def run_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Runs the Llama model with a simple prompt\n",
    "    Args:\n",
    "        prompt: Input prompt text\n",
    "    Returns:\n",
    "        Generated response from the model\n",
    "    HINTS: \n",
    "      1) Create the 'messages' list with system and user roles.\n",
    "      2) Call llm_for_chat.create_chat_completion() with those messages.\n",
    "      3) Return the text from response[\"choices\"][0][\"message\"][\"content\"].\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    messages = []\n",
    "    messages.append({'role':'system', 'content':system_prompt})\n",
    "    messages.append({'role':'user', 'content':prompt})\n",
    "    response = llm_for_chat.create_chat_completion(messages)\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_with_pdf_knowledge(user_query, k=3):\n",
    "    \"\"\"\n",
    "    Runs the Llama model with context from similar PDF chunks to provide informed answers\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The user's question to be answered\n",
    "        k (int): Number of similar chunks to retrieve from the PDF (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated response incorporating knowledge from the PDF context\n",
    "    HINTS:\n",
    "      1) Use search_similar_chunks to retrieve top_k chunks.\n",
    "      2) Combine them into a single 'context' string.\n",
    "      3) Construct a prompt that includes the PDF context plus the user query.\n",
    "      4) Pass that prompt to run_llm(prompt).\n",
    "      5) Return the final response text.\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    search_results = search_similar_chunks(user_query,k)\n",
    "    context = \"\"\n",
    "    for (chunk, dist) in (search_results):\n",
    "        context += \" \" + chunk\n",
    "    prompt = context + \" \" + user_query\n",
    "    response = run_llm(prompt)\n",
    "    return response\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.4293613e+03 3.4028235e+38 3.4028235e+38]] [[ 0 -1 -1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  453645.63 ms\n",
      "llama_print_timings:      sample time =    1177.63 ms /   705 runs   (    1.67 ms per token,   598.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  499960.82 ms /   610 tokens (  819.61 ms per token,     1.22 tokens per second)\n",
      "llama_print_timings:        eval time =  426464.56 ms /   704 runs   (  605.77 ms per token,     1.65 tokens per second)\n",
      "llama_print_timings:       total time =  950265.16 ms /  1314 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Whats is the avg cgm values for my patient?\n",
      "A: Rephrased question: What are the average CGM (Continuous Glucose Monitoring) values for a 57-year-old male with Type 2 Diabetes Mellitus (T2DM), considering his medical history, medications, and lifestyle factors?\n",
      "\n",
      "Broken down into subquestions:\n",
      "\n",
      "1. What is the patient's current glycemic control based on his HbA1c and fasting glucose levels?\n",
      "2. How do his medication regimen and non-adherence issues affect his blood sugar control?\n",
      "3. What are the potential implications of his comorbidities, such as hypertension, dyslipidemia, obesity, diabetic retinopathy, diabetic neuropathy, and chronic kidney disease on his glycemic control?\n",
      "\n",
      "Step-by-step answer:\n",
      "\n",
      "1. Based on the patient's HbA1c level (8.3%) and fasting glucose level (160 mg/dL), his current glycemic control is above target.\n",
      "2. The patient's medication regimen includes metformin, insulin glargine, lisinopril, and atorvastatin, which are all relevant to his T2DM management. However, his non-adherence issues with insulin doses (missing 2-3 days per week) and poor diet management may be contributing to his suboptimal glycemic control.\n",
      "3. The patient's comorbidities can also impact his glycemic control:\n",
      "\t* Hypertension: Controlled with medication, but uncontrolled hypertension can worsen glucose metabolism.\n",
      "\t* Dyslipidemia: Partially controlled, which may increase the risk of cardiovascular events and affect glucose metabolism.\n",
      "\t* Obesity: BMI 31 kg/m¬≤, limited physical activity, and poor diet management contribute to insulin resistance and worsened glycemic control.\n",
      "\t* Diabetic Retinopathy: Mild background retinopathy may not significantly impact glycemic control but can lead to vision complications if left unmanaged.\n",
      "\t* Diabetic Neuropathy: Tingling and numbness in feet can make it difficult for the patient to engage in physical activity, exacerbating his sedentary lifestyle.\n",
      "\t* Chronic Kidney Disease: Stage 2 (GFR 65 mL/min/1.73 m¬≤) may require adjustments to medication dosages or glucose monitoring.\n",
      "\n",
      "Average CGM values:\n",
      "\n",
      "Unfortunately, without actual CGM data, it is challenging to provide specific average CGM values for this patient. However, we can discuss the general trends and ranges of CGM values in patients with T2DM.\n",
      "\n",
      "CGM values typically range from 70-180 mg/dL, with an average value around 100-120 mg/dL. In patients with poor glycemic control, such as this patient, CGM values may be higher, often above 150-200 mg/dL.\n",
      "\n",
      "_See also:_ [Continuous Glucose Monitoring in Type 2 Diabetes](https://scholar.google.com/scholar?q=continuous+glucose+monitoring+in+type+2+diabetes)\n",
      "[CGM for T2DM Management](https://www.google.com/search?q=continuous+glucose+monitoring+for+type+2+diabetes+management)\n",
      "\n",
      "_You may also enjoy:_ [T2DM Management Strategies](https://www.google.com/search?q=type+2+diabetes+mellitus+management+strategies)\n",
      "[Patient Education for T2DM](https://www.google.com/search?q=patient+education+for+type+2+diabetes+mellitus)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  453645.63 ms\n",
      "llama_print_timings:      sample time =    1198.98 ms /   469 runs   (    2.56 ms per token,   391.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13440.99 ms /    16 tokens (  840.06 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:        eval time =  371943.03 ms /   468 runs   (  794.75 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time =  407714.85 ms /   484 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Whats is the avg cgm values for my patient?\n",
      "A: I've rephrased your question to better anticipate your needs: \"What are the average CGM (Continuous Glucose Monitoring) values for my patient, and how can I interpret them?\"\n",
      "\n",
      "To provide a comprehensive answer, let's break down the question into subquestions:\n",
      "\n",
      "1. What are normal CGM values?\n",
      "2. How do I interpret CGM values in the context of my patient's health?\n",
      "\n",
      "**Normal CGM Values:**\n",
      "\n",
      "CGM values vary depending on the device and manufacturer, but generally, a normal range for glucose levels is between 70-140 mg/dL (3.9-7.8 mmol/L) for adults.\n",
      "\n",
      "* _See also:_ [Continuous Glucose Monitoring Systems](https://scholar.google.com/scholar?q=continuous+glucose+monitoring+systems)\n",
      "  üîç[General information](https://www.google.com/search?q=continuous+glucose+monitoring)\n",
      "\n",
      "**Interpreting CGM Values for Your Patient:**\n",
      "\n",
      "To interpret CGM values, consider the following factors:\n",
      "\n",
      "1. **Time of day:** Glucose levels tend to be higher in the morning and lower at night.\n",
      "2. **Meal times:** Postprandial (after meals) glucose levels may be higher than fasting levels.\n",
      "3. **Physical activity:** Exercise can cause temporary increases or decreases in glucose levels, depending on the intensity and duration.\n",
      "4. **Medications:** Certain medications, such as insulin or oral antidiabetic agents, can affect glucose levels.\n",
      "\n",
      "For your patient, consider the following steps to interpret CGM values:\n",
      "\n",
      "1. Review the overall trend: Look for patterns of high or low glucose levels over a 24-hour period.\n",
      "2. Identify peak and trough levels: Note the highest and lowest glucose levels during the day.\n",
      "3. Consider the time of day and meal times: Adjust your interpretation based on the patient's eating schedule and physical activity.\n",
      "4. Monitor for trends and anomalies: Look for consistent patterns or unusual fluctuations that may indicate changes in the patient's health.\n",
      "\n",
      "**You may also enjoy:** [Understanding CGM Data](https://www.google.com/search?q=understanding+cgm+data)\n",
      "\n",
      "Remember to consult with your healthcare provider or a certified diabetes educator for personalized guidance on interpreting CGM values and developing a treatment plan for your patient.\n"
     ]
    }
   ],
   "source": [
    "new_query = \"Whats is the avg cgm values for my patient?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.0036055e+03 3.4028235e+38 3.4028235e+38]] [[ 0 -1 -1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  453645.63 ms\n",
      "llama_print_timings:      sample time =    1807.11 ms /   710 runs   (    2.55 ms per token,   392.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =  123299.17 ms /   409 tokens (  301.46 ms per token,     3.32 tokens per second)\n",
      "llama_print_timings:        eval time =  514756.61 ms /   709 runs   (  726.03 ms per token,     1.38 tokens per second)\n",
      "llama_print_timings:       total time =  689876.48 ms /  1118 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the patient's last recorded blood pressure?\n",
      "A: Rephrased question: What are the key factors contributing to this patient's suboptimal glycemic control, and how can we improve their management plan?\n",
      "\n",
      "Broken down into subquestions:\n",
      "\n",
      "1. What are the primary causes of the patient's poor glycemic control?\n",
      "2. How does the patient's medication regimen contribute to their current condition?\n",
      "3. Are there any lifestyle factors that may be exacerbating the patient's T2DM?\n",
      "4. What is the impact of the patient's comorbidities on their overall health and glycemic control?\n",
      "\n",
      "Step-by-step answers:\n",
      "\n",
      "1. Primary causes of poor glycemic control:\n",
      "\t* The patient's last HbA1c level was 8.3%, indicating above-target glucose control.\n",
      "\t* Fasting glucose levels were 160 mg/dL, which is higher than the target range.\n",
      "\t* Non-adherence to insulin doses (2-3 days per week) and poor diet management are significant contributors to suboptimal glycemic control.\n",
      "\n",
      "_See also:_ [Glycemic control in T2DM](https://scholar.google.com/scholar?q=glycemic+control+in+t2dm)\n",
      "[Diabetes self-management education](https://www.google.com/search?q=diabetes+self-management+education)\n",
      "\n",
      "2. Medication regimen:\n",
      "\t* The patient is taking metformin 1000 mg twice daily, insulin glargine 20 units at bedtime, lisinopril 10 mg once daily for hypertension, and atorvastatin 40 mg once daily for hyperlipidemia.\n",
      "\t* However, the patient's medication regimen may not be optimized for their specific needs.\n",
      "\n",
      "_See also:_ [Optimizing medication regimens in T2DM](https://scholar.google.com/scholar?q=optimizing+medication+regimens+in+t2dm)\n",
      "[Medication adherence in T2DM](https://www.google.com/search?q=medication+adherence+in+t2dm)\n",
      "\n",
      "3. Lifestyle factors:\n",
      "\t* The patient consumes a high-carbohydrate diet and has irregular meal patterns.\n",
      "\t* They have a very low activity level, walking for only 10 minutes a day.\n",
      "\n",
      "_See also:_ [Dietary management in T2DM](https://scholar.google.com/scholar?q=dietary+management+in+t2dm)\n",
      "[Physical activity and exercise in T2DM](https://www.google.com/search?q=physical+activity+and+exercise+in+t2dm)\n",
      "\n",
      "4. Comorbidities:\n",
      "\t* The patient has hypertension, dyslipidemia, obesity, diabetic retinopathy, diabetic neuropathy, and chronic kidney disease (CKD) stage 2.\n",
      "\t* These comorbidities can impact the patient's overall health and glycemic control.\n",
      "\n",
      "_See also:_ [Comorbidities in T2DM](https://scholar.google.com/scholar?q=comorbidities+in+t2dm)\n",
      "[Managing comorbidities in T2DM](https://www.google.com/search?q=managing+comorbidities+in+t2dm)\n",
      "\n",
      "Last recorded blood pressure: Not mentioned in the patient's chart.\n",
      "\n",
      "_You may also enjoy:_ [T2DM management guidelines](https://scholar.google.com/scholar?q=t2dm+management+guidelines)\n",
      "[Diabetes education resources](https://www.google.com/search?q=diabetes+education+resources)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  453645.63 ms\n",
      "llama_print_timings:      sample time =    1110.48 ms /   482 runs   (    2.30 ms per token,   434.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7212.25 ms /    15 tokens (  480.82 ms per token,     2.08 tokens per second)\n",
      "llama_print_timings:        eval time =  253573.36 ms /   481 runs   (  527.18 ms per token,     1.90 tokens per second)\n",
      "llama_print_timings:       total time =  289470.24 ms /   496 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the patient's last recorded blood pressure?\n",
      "A: I've rephrased your question to better anticipate your needs: \"Can you provide me with information about a patient's most recent blood pressure reading?\"\n",
      "\n",
      "To break this down further, I'll ask some subquestions:\n",
      "\n",
      "1. What type of patient are we referring to (e.g., pediatric, adult, geriatric)?\n",
      "2. Is the patient being monitored in a hospital or outpatient setting?\n",
      "3. Are there any specific medical conditions or medications that may affect blood pressure?\n",
      "\n",
      "Assuming you're looking for general information on a patient's last recorded blood pressure, here's a step-by-step guide:\n",
      "\n",
      "**Step 1: Determine the patient's vital signs**\n",
      "\n",
      "* Blood pressure is typically measured in mmHg (millimeters of mercury) and includes two numbers: systolic pressure (the top number) and diastolic pressure (the bottom number).\n",
      "* Normal blood pressure ranges vary depending on age, sex, and other factors. For adults, a normal range is usually considered 90-120 mmHg for systolic and 60-80 mmHg for diastolic.\n",
      "\n",
      "**Step 2: Review the patient's medical history**\n",
      "\n",
      "* If the patient has a pre-existing condition or takes medications that affect blood pressure, their readings may be higher or lower than normal.\n",
      "* Certain conditions like hypertension (high blood pressure) or hypotension (low blood pressure) can impact overall health and require monitoring.\n",
      "\n",
      "**Step 3: Check for any recent medical records**\n",
      "\n",
      "* If the patient is being monitored in a hospital or clinic setting, medical staff will typically record their vital signs, including blood pressure, at regular intervals.\n",
      "* Reviewing these records can provide valuable information about the patient's current blood pressure and any changes over time.\n",
      "\n",
      "**Additional resources:**\n",
      "\n",
      "_See also:_ Hypertension management, blood pressure monitoring techniques, and patient education on blood pressure control.\n",
      "\n",
      "üìö[Research topic articles](https://scholar.google.com/scholar?q=patient+blood+pressure+monitoring)\n",
      "\n",
      "üîç[General information](https://www.google.com/search?q=blood+pressure+management+tips)\n",
      "\n",
      "_You may also enjoy:_ Understanding the importance of blood pressure control, hypertension diagnosis and treatment options, and patient education on healthy lifestyle choices.\n",
      "\n",
      "üåü[Explore more](https://www.google.com/search?q=healthy+lifestyle+choices+blood+pressure)\n"
     ]
    }
   ],
   "source": [
    "new_query = \"What is the patient's last recorded blood pressure?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
